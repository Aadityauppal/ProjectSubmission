{"name":"Projectsubmission","tagline":"Machine Learning Project","body":"# \"Predicting Activity Quality from Activity Monitors\"\r\n# \"Aaditya Uppal\"\r\n# \"Thursday, July 23, 2015\"\r\n---\r\n\r\nThe goal of this assignment is to predict and classify the quality of exercise of 6 subjects based on 4 different activity monitors. A training dataset is available to build and train the predictive model and then make predictions on the test set.\r\n\r\n## Reading Data\r\n\r\nWe first grab the training and test data provided for this assignment.\r\n\r\n```{r, warning=FALSE, message=FALSE}\r\ntrainData <- read.csv(file = \"./pml-training.csv\", header = TRUE, sep = \",\", na.strings = \"#DIV/0!\")\r\ntestData <- read.csv(file = \"./pml-testing.csv\", header = TRUE, sep = \",\", na.strings = \"#DIV/0!\")\r\n```\r\n\r\n## Back up of Raw Data\r\n\r\nBoth the datasets are backed up for later use. As both the datasets are raw and need to be cleaned, it is helpful to backup the datasets as you'll see.\r\n\r\n```{r, warning=FALSE, message=FALSE}\r\ntrainDataRD <- trainData\r\ntestDataRD <- testData\r\n```\r\n\r\n## Identifying variables not required for prediction\r\n\r\nA quick look into the data shows there are 160 variables. 38 different readings for each sensor account for 152(38*4) variables. Not all the variables may be required for building the model. The monitors data is recorded into 152 variables. The 1st 7 variables have information specific to the subject or the timestamp at which data was recorded. These variables are not needed for building the model and prediction. We drop these variables and do not include them any further.\r\n\r\n```{r, warning=FALSE, message=FALSE}\r\nlibrary(dplyr)\r\ntrainData <- select(trainData, -c(1:7))\r\ntestData <- select(testData, -c(1:7))\r\n```\r\n\r\nWhile checking the data for missing values, we observe that lot of variables have missing values or other strings recorded as NAs. We first attempt to understand which variables and what percentage of them constitute NAs and then decide how to tackle the NA imputation.\r\n\r\n```{r, warning=FALSE, message=FALSE}\r\n# Converting all variables into numeric class to identify missing values.\r\n  trainData[, 1:152] <- sapply(trainData[, 1:152], function(x) as.numeric(as.character(x)))\r\n  testData[, 1:152] <- sapply(testData[, 1:152], function(x) as.numeric(as.character(x)))\r\n# Summation of observations for each variable\r\n  varsNAall <- apply(trainData[, 1:152], 2, mean)\r\n  varsNA1all <- apply(trainData[, 1:152], 2, function(x) mean(x, na.rm = TRUE))\r\n# If all observations are NA, we get mean as NaN\r\n# Recording index for variables with all NAs \r\n  varIndNAall <- grep(\"NaN\", varsNA1all)\r\n# Summation of observations for each variable for test data\r\n  varsNAtestall <- apply(testData[, 1:152], 2, mean)\r\n  varsNAtest1all <- apply(testData[, 1:152], 2, function(x) mean(x, na.rm = TRUE))\r\n# Recording index for variables with all NAs for test data\r\n  varIndNAtestall <- grep(\"NaN\", varsNAtest1all)\r\n# We can also check whether the all NAs index for training set is a subset of that for the test set\r\n## sum(varIndNAtestall %in% varIndNAall) == length(varIndNAall)\r\n# Removing variables which are all NAs in the training set\r\n  trainData <- trainData[, -varIndNAall]\r\n  testData <- testData[, -varIndNAall]\r\n# Summation of observations for remaining variables\r\n  varsNA <- apply(trainData[, 1:146], 2, mean)\r\n  varsNA1 <- apply(trainData[, 1:146], 2, function(x) mean(x, na.rm = TRUE))\r\n# Recording index for variables with at least 1 NA\r\n  varIndNA <- which(varsNA %in% as.numeric(as.character(\"\")))\r\n# Summation of observations for remaining variables for test data\r\n  varsNAtest <- apply(testData[, 1:146], 2, mean)\r\n  varsNA1test <- apply(trainData[, 1:146], 2, function(x) mean(x, na.rm = TRUE))\r\n# Recording index for variables with at least 1 NA in test data\r\n  varIndNAtest <- which(varsNAtest %in% as.numeric(as.character(\"\")))\r\n# Variables containing NAs for test and training data are identical\r\n## identical(varIndNA, varIndNAtest) # Returns TRUE\r\n\r\n# Separating the variables containing NAs to check %age of NA values\r\n  dtySet <- trainData[, varIndNA]\r\n  sum(is.na(dtySet)) / (dim(dtySet)[1] * dim(dtySet)[2]) \r\n## Returns 98% NAs in total\r\n  propNNAs <- apply(dtySet, 2, function(x) sum(!is.na(x))/length(x))\r\n#  max(propNNAs) \r\n## not more than 2% values Non-NAs for any of the variables\r\n# We can remove these variables to get a cleaner dataset containing 52 variables and the categorical outcome\r\n  clnSet <- trainData[, -varIndNA]\r\n\r\n# Separating the variables containing NAs to check %age of NA values in the test set\r\n  dtySetTst <- testData[, varIndNAtest]\r\n  sum(is.na(dtySetTst)) / (dim(dtySetTst)[1] * dim(dtySetTst)[2]) \r\n## Returns 100% NAs\r\n# We can remove these variables to get a cleaner dataset containing 52 variables and the problem id for the test set\r\n  clnSetTst <- testData[, -varIndNAtest]\r\n\r\n## Imputation is not carried out for NAs as large number of similar values in each variable will introduce bias\r\n```\r\n\r\nThe subsetting is now applied on the backed up raw datasets to obtain the cleaner dataset. The dataset used for building the model contains 52 variables and the classifier outcome.\r\n\r\n```{r, warning=FALSE, message=FALSE}\r\nlibrary(dplyr)\r\n  trnDat <- select(trainDataRD, -c(1:7))\r\n  tstDat <- select(testDataRD, -c(1:7))\r\n\r\n  trnDat <- trnDat[, -varIndNAall]\r\n  tstDat <- tstDat[, -varIndNAall]\r\n\r\n  trnDat <- trnDat[, -varIndNA]\r\n  tstDat <- tstDat[, -varIndNAtest]\r\n```\r\n\r\n## Building the Prediction model\r\n\r\n*Classification Trees and Random Forests are suitable for building prediction models for classification based outcomes*. For this assignment, Random forests is used as it provides high accuracy and can handle large number of variables without variable deletion.\r\n\r\n'randomForest' is the function used to implement this task. The arguments provided to the randomForest function are the number of trees to be grown, number of variables randomly sampled at each split and the importance of predictors. \r\n\r\nNumber of trees can be set even higher but the error rates are more or less constant beyond 100 trees. Even with mtry = 3 pretty high accuracy is achieved. We want the importance of predictors to be assessed so setting the 'importance' argument as TRUE. \r\n\r\n```{r, warning=FALSE, message=FALSE}\r\nlibrary(randomForest)\r\n  set.seed(1234)\r\n  modRFfin <- randomForest(classe ~ ., data = trnDat, ntree = 100, mtry = 3, importance = TRUE)\r\n  modRFfin\r\n```\r\n\r\n## Cross-validation and Out of sample error\r\n\r\nEach tree in a random forest is constructed using a different bootstrap sample from the original data. About one-third of the cases are left out of the bootstrap sample and not used in the construction of the kth tree. Hence, **there is no need of cross-validation when using random forests**. The out of bag (OOB) error rate is the estimate on the out of bag samples. In this way, a test set classification is obtained for each case in about one-third of the trees. So **the OOB error rate is an unbiased estimate of the test set error**. Based on our model, **the expected out of sample error is less than 0.5%**. \r\n\r\n## Error rate and Variable Importance plots\r\n\r\nLooking at how error rate behaves as the trees are built gives a good estimate of whether the solution is converging. We plot the error rates here and find that error rates for each of the outcome is close to 1% at around the 30th tree. OOB error rate is also plotted alongwith the error rate for each outcome.\r\n\r\nIt is also useful to look at the important variables as per our model. We have plotted the top 10 variables which are important w.r.t. the mean decrease in accuracy and the mean decrease in Gini Index.\r\n\r\n```{r, warning=FALSE, message=FALSE}\r\n## Error rates plot\r\n  plot(modRFfin, type = \"l\")\r\n  legend(\"topright\", colnames(modRFfin$err.rate), col=1:6, cex=0.8, fill=1:6)\r\n\r\n## Plotting variable importance\r\n  varImpPlot(modRFfin, n.var = 10, scale = FALSE)\r\n```\r\n\r\n## Submission for test data\r\n\r\nThe model is applied to the test cases to predict the outcomes using the predict function as shown below. **Correct outcomes are predicted for each test case**.\r\n\r\n```{r, eval=FALSE}\r\nans <- predict(modRFfin, newdata = tstDat[, -53])\r\n\r\npml_write_files = function(x){\r\n  n = length(x)\r\n  for(i in 1:n){\r\n    filename = paste0(\"problem_id_\",i,\".txt\")\r\n    write.table(x[i],file=filename,quote=FALSE,row.names=FALSE,col.names=FALSE)\r\n  }\r\n}\r\n\r\npll_write_files(ans)\r\n```\r\n\r\n## Conclusions\r\n\r\n*A robust model was built using Random Forests and found to predict correctly on the test cases. Using this model, the expected out of sample error is less than 0.5%*","google":"","note":"Don't delete this file! It's used internally to help with page regeneration."}