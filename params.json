{"name":"Projectsubmission","tagline":"Machine Learning Project","body":"<div id=\"header\">\r\n<h1 class=\"title\">Predicting Activity Quality from Activity Monitors</h1>\r\n<h4 class=\"author\"><em>Aaditya Uppal</em></h4>\r\n<h4 class=\"date\"><em>Thursday, July 16, 2015</em></h4>\r\n</div>\r\n\r\n\r\n<p>The goal of this assignment is to predict and classify the quality of exercise of 6 subjects based on 4 different activity monitors. A training dataset is available to build and train the predictive model and then make predictions on the test set.</p>\r\n<div id=\"reading-data\" class=\"section level1\">\r\n<h1>Reading Data</h1>\r\n<p>We first grab the training and test data provided for this assignment.</p>\r\n<pre class=\"r\"><code>trainData &lt;- read.csv(file = &quot;./pml-training.csv&quot;, header = TRUE, sep = &quot;,&quot;, na.strings = &quot;#DIV/0!&quot;)\r\ntestData &lt;- read.csv(file = &quot;./pml-testing.csv&quot;, header = TRUE, sep = &quot;,&quot;, na.strings = &quot;#DIV/0!&quot;)</code></pre>\r\n</div>\r\n<div id=\"back-up-of-raw-data\" class=\"section level1\">\r\n<h1>Back up of Raw Data</h1>\r\n<p>Both the datasets are backed up for later use. As both the datasets are raw and need to be cleaned, it is helpful to backup the datasets as you’ll see.</p>\r\n<pre class=\"r\"><code>trainDataRD &lt;- trainData\r\ntestDataRD &lt;- testData</code></pre>\r\n</div>\r\n<div id=\"identifying-variables-not-required-for-prediction\" class=\"section level1\">\r\n<h1>Identifying variables not required for prediction</h1>\r\n<p>A quick look into the data shows there are 160 variables. 38 different readings for each sensor account for 152(38*4) variables. Not all the variables may be required for building the model. The monitors data is recorded into 152 variables. The 1st 7 variables have information specific to the subject or the timestamp at which data was recorded. These variables are not needed for building the model and prediction. We drop these variables and do not include them any further.</p>\r\n<pre class=\"r\"><code>library(dplyr)\r\ntrainData &lt;- select(trainData, -c(1:7))\r\ntestData &lt;- select(testData, -c(1:7))</code></pre>\r\n<p>While checking the data for missing values, we observe that lot of variables have missing values or other strings recorded as NAs. We first attempt to understand which variables and what percentage of them constitute NAs and then decide how to tackle the NA imputation.</p>\r\n<pre class=\"r\"><code># Converting all variables into numeric class to identify missing values.\r\n  trainData[, 1:152] &lt;- sapply(trainData[, 1:152], function(x) as.numeric(as.character(x)))\r\n  testData[, 1:152] &lt;- sapply(testData[, 1:152], function(x) as.numeric(as.character(x)))\r\n# Summation of observations for each variable\r\n  varsNAall &lt;- apply(trainData[, 1:152], 2, mean)\r\n  varsNA1all &lt;- apply(trainData[, 1:152], 2, function(x) mean(x, na.rm = TRUE))\r\n# If all observations are NA, we get mean as NaN\r\n# Recording index for variables with all NAs \r\n  varIndNAall &lt;- grep(&quot;NaN&quot;, varsNA1all)\r\n# Summation of observations for each variable for test data\r\n  varsNAtestall &lt;- apply(testData[, 1:152], 2, mean)\r\n  varsNAtest1all &lt;- apply(testData[, 1:152], 2, function(x) mean(x, na.rm = TRUE))\r\n# Recording index for variables with all NAs for test data\r\n  varIndNAtestall &lt;- grep(&quot;NaN&quot;, varsNAtest1all)\r\n# We can also check whether the all NAs index for training set is a subset of that for the test set\r\n## sum(varIndNAtestall %in% varIndNAall) == length(varIndNAall)\r\n# Removing variables which are all NAs in the training set\r\n  trainData &lt;- trainData[, -varIndNAall]\r\n  testData &lt;- testData[, -varIndNAall]\r\n# Summation of observations for remaining variables\r\n  varsNA &lt;- apply(trainData[, 1:146], 2, mean)\r\n  varsNA1 &lt;- apply(trainData[, 1:146], 2, function(x) mean(x, na.rm = TRUE))\r\n# Recording index for variables with at least 1 NA\r\n  varIndNA &lt;- which(varsNA %in% as.numeric(as.character(&quot;&quot;)))\r\n# Summation of observations for remaining variables for test data\r\n  varsNAtest &lt;- apply(testData[, 1:146], 2, mean)\r\n  varsNA1test &lt;- apply(trainData[, 1:146], 2, function(x) mean(x, na.rm = TRUE))\r\n# Recording index for variables with at least 1 NA in test data\r\n  varIndNAtest &lt;- which(varsNAtest %in% as.numeric(as.character(&quot;&quot;)))\r\n# Variables containing NAs for test and training data are identical\r\n## identical(varIndNA, varIndNAtest) # Returns TRUE\r\n\r\n# Separating the variables containing NAs to check %age of NA values\r\n  dtySet &lt;- trainData[, varIndNA]\r\n  sum(is.na(dtySet)) / (dim(dtySet)[1] * dim(dtySet)[2]) </code></pre>\r\n<pre><code>## [1] 0.9798869</code></pre>\r\n<pre class=\"r\"><code>## Returns 98% NAs in total\r\n  propNNAs &lt;- apply(dtySet, 2, function(x) sum(!is.na(x))/length(x))\r\n#  max(propNNAs) \r\n## not more than 2% values Non-NAs for any of the variables\r\n# We can remove these variables to get a cleaner dataset containing 52 variables and the categorical outcome\r\n  clnSet &lt;- trainData[, -varIndNA]\r\n\r\n# Separating the variables containing NAs to check %age of NA values in the test set\r\n  dtySetTst &lt;- testData[, varIndNAtest]\r\n  sum(is.na(dtySetTst)) / (dim(dtySetTst)[1] * dim(dtySetTst)[2]) </code></pre>\r\n<pre><code>## [1] 1</code></pre>\r\n<pre class=\"r\"><code>## Returns 100% NAs\r\n# We can remove these variables to get a cleaner dataset containing 52 variables and the problem id for the test set\r\n  clnSetTst &lt;- testData[, -varIndNAtest]\r\n\r\n## Imputation is not carried out for NAs as large number of similar values in each variable will introduce bias</code></pre>\r\n<p>The subsetting is now applied on the backed up raw datasets to obtain the cleaner dataset. The dataset used for building the model contains 52 variables and the classifier outcome.</p>\r\n<pre class=\"r\"><code>library(dplyr)\r\n  trnDat &lt;- select(trainDataRD, -c(1:7))\r\n  tstDat &lt;- select(testDataRD, -c(1:7))\r\n\r\n  trnDat &lt;- trnDat[, -varIndNAall]\r\n  tstDat &lt;- tstDat[, -varIndNAall]\r\n\r\n  trnDat &lt;- trnDat[, -varIndNA]\r\n  tstDat &lt;- tstDat[, -varIndNAtest]</code></pre>\r\n</div>\r\n<div id=\"building-the-prediction-model\" class=\"section level1\">\r\n<h1>Building the Prediction model</h1>\r\n<p><em>Classification Trees and Random Forests are suitable for building prediction models for classification based outcomes</em>. For this assignment, Random forests is used as it provides high accuracy and can handle large number of variables without variable deletion.</p>\r\n<p>‘randomForest’ is the function used to implement this task. The arguments provided to the randomForest function are the number of trees to be grown, number of variables randomly sampled at each split and the importance of predictors.</p>\r\n<p>Number of trees can be set even higher but the error rates are more or less constant beyond 100 trees. Even with mtry = 3 pretty high accuracy is achieved. We want the importance of predictors to be assessed so setting the ‘importance’ argument as TRUE.</p>\r\n<pre class=\"r\"><code>library(randomForest)\r\n  set.seed(1234)\r\n  modRFfin &lt;- randomForest(classe ~ ., data = trnDat, ntree = 100, mtry = 3, importance = TRUE)\r\n  modRFfin</code></pre>\r\n<pre><code>## \r\n## Call:\r\n##  randomForest(formula = classe ~ ., data = trnDat, ntree = 100,      mtry = 3, importance = TRUE) \r\n##                Type of random forest: classification\r\n##                      Number of trees: 100\r\n## No. of variables tried at each split: 3\r\n## \r\n##         OOB estimate of  error rate: 0.45%\r\n## Confusion matrix:\r\n##      A    B    C    D    E  class.error\r\n## A 5578    2    0    0    0 0.0003584229\r\n## B   17 3776    4    0    0 0.0055306821\r\n## C    0   14 3404    4    0 0.0052600818\r\n## D    0    0   35 3177    4 0.0121268657\r\n## E    0    0    1    7 3599 0.0022179096</code></pre>\r\n</div>\r\n<div id=\"cross-validation-and-out-of-sample-error\" class=\"section level1\">\r\n<h1>Cross-validation and Out of sample error</h1>\r\n<p>Each tree in a random forest is constructed using a different bootstrap sample from the original data. About one-third of the cases are left out of the bootstrap sample and not used in the construction of the kth tree. Hence, <strong>there is no need of cross-validation when using random forests</strong>. The out of bag (OOB) error rate is the estimate on the out of bag samples. In this way, a test set classification is obtained for each case in about one-third of the trees. So <strong>the OOB error rate is an unbiased estimate of the test set error</strong>. Based on our model, <strong>the expected out of sample error is less than 0.5%</strong>.</p>\r\n</div>\r\n<div id=\"error-rate-and-variable-importance-plots\" class=\"section level1\">\r\n<h1>Error rate and Variable Importance plots</h1>\r\n<p>Looking at how error rate behaves as the trees are built gives a good estimate of whether the solution is converging. We plot the error rates here and find that error rates for each of the outcome is close to 1% at around the 30th tree. OOB error rate is also plotted alongwith the error rate for each outcome.</p>\r\n<p>It is also useful to look at the important variables as per our model. We have plotted the top 10 variables which are important w.r.t. the mean decrease in accuracy and the mean decrease in Gini Index.</p>\r\n<p><a href=\"https://github.com/Aadityauppal/ProjectSubmission/blob/master/Error%20Rate%20Plot.png\">Error Rate plot</a></p>\r\n<pre class=\"r\"><code>## Error rates plot\r\n  plot(modRFfin, type = &quot;l&quot;)\r\n  legend(&quot;topright&quot;, colnames(modRFfin$err.rate), col=1:6, cex=0.8, fill=1:6)</code></pre>\r\n<p><a href=\"https://github.com/Aadityauppal/ProjectSubmission/blob/master/VariableImportancePlot.png\">Variable Importance Plot</a></p>\r\n<pre class=\"r\"><code>## Plotting variable importance\r\n  varImpPlot(modRFfin, n.var = 10, scale = FALSE)</code></pre>\r\n<div id=\"submission-for-test-data\" class=\"section level2\">\r\n<h2>Submission for test data</h2>\r\n<p>The model is applied to the test cases to predict the outcomes using the predict function as shown below. <strong>Correct outcomes are predicted for each test case</strong>.</p>\r\n<pre class=\"r\"><code>ans &lt;- predict(modRFfin, newdata = tstDat[, -53])\r\n\r\npml_write_files = function(x){\r\n  n = length(x)\r\n  for(i in 1:n){\r\n    filename = paste0(&quot;problem_id_&quot;,i,&quot;.txt&quot;)\r\n    write.table(x[i],file=filename,quote=FALSE,row.names=FALSE,col.names=FALSE)\r\n  }\r\n}\r\n\r\npll_write_files(ans)</code></pre>\r\n</div>\r\n<div id=\"conclusions\" class=\"section level2\">\r\n<h2>Conclusions</h2>\r\n<p><em>A robust model was built using Random Forests and found to predict correctly on the test cases. Using this model, the expected out of sample error is less than 0.5%</em></p>\r\n</div>\r\n</div>\r\n\r\n\r\n</div>\r\n\r\n<script>\r\n// add bootstrap table styles to pandoc tables\r\n$(document).ready(function () {\r\n  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');\r\n});\r\n</script>\r\n\r\n<!-- dynamically load mathjax for compatibility with self-contained -->\r\n<script>\r\n  (function () {\r\n    var script = document.createElement(\"script\");\r\n    script.type = \"text/javascript\";\r\n    script.src  = \"https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML\";\r\n    document.getElementsByTagName(\"head\")[0].appendChild(script);\r\n  })();\r\n</script>\r\n\r\n</body>\r\n</html>","google":"","note":"Don't delete this file! It's used internally to help with page regeneration."}