<!DOCTYPE html>
<html>

  <head>
    <meta charset='utf-8'>
    <meta http-equiv="X-UA-Compatible" content="chrome=1">
    <meta name="description" content="Projectsubmission : Machine Learning Project">

    <link rel="stylesheet" type="text/css" media="screen" href="stylesheets/stylesheet.css">

    <title>Projectsubmission</title>
  </head>

  <body>

    <!-- HEADER -->
    <div id="header_wrap" class="outer">
        <header class="inner">
          <a id="forkme_banner" href="https://github.com/Aadityauppal/ProjectSubmission">View on GitHub</a>

          <h1 id="project_title">Projectsubmission</h1>
          <h2 id="project_tagline">Machine Learning Project</h2>

            <section id="downloads">
              <a class="zip_download_link" href="https://github.com/Aadityauppal/ProjectSubmission/zipball/master">Download this project as a .zip file</a>
              <a class="tar_download_link" href="https://github.com/Aadityauppal/ProjectSubmission/tarball/master">Download this project as a tar.gz file</a>
            </section>
        </header>
    </div>

    <!-- MAIN CONTENT -->
    <div id="main_content_wrap" class="outer">
      <section id="main_content" class="inner">
        <hr>

<p>title: "Predicting Activity Quality from Activity Monitors"
author: "Aaditya Uppal"
date: "Thursday, July 16, 2015"</p>

<h2>
<a id="output-html_document" class="anchor" href="#output-html_document" aria-hidden="true"><span class="octicon octicon-link"></span></a>output: html_document</h2>

<p>The goal of this assignment is to predict and classify the quality of exercise of 6 subjects based on 4 different activity monitors. A training dataset is available to build and train the predictive model and then make predictions on the test set.</p>

<h1>
<a id="reading-data" class="anchor" href="#reading-data" aria-hidden="true"><span class="octicon octicon-link"></span></a>Reading Data</h1>

<p>We first grab the training and test data provided for this assignment.</p>

<pre lang="r,"><code>trainData &lt;- read.csv(file = "./pml-training.csv", header = TRUE, sep = ",", na.strings = "#DIV/0!")
testData &lt;- read.csv(file = "./pml-testing.csv", header = TRUE, sep = ",", na.strings = "#DIV/0!")
</code></pre>

<h1>
<a id="back-up-of-raw-data" class="anchor" href="#back-up-of-raw-data" aria-hidden="true"><span class="octicon octicon-link"></span></a>Back up of Raw Data</h1>

<p>Both the datasets are backed up for later use. As both the datasets are raw and need to be cleaned, it is helpful to backup the datasets as you'll see.</p>

<pre lang="r,"><code>trainDataRD &lt;- trainData
testDataRD &lt;- testData
</code></pre>

<h1>
<a id="identifying-variables-not-required-for-prediction" class="anchor" href="#identifying-variables-not-required-for-prediction" aria-hidden="true"><span class="octicon octicon-link"></span></a>Identifying variables not required for prediction</h1>

<p>A quick look into the data shows there are 160 variables. 38 different readings for each sensor account for 152(38*4) variables. Not all the variables may be required for building the model. The monitors data is recorded into 152 variables. The 1st 7 variables have information specific to the subject or the timestamp at which data was recorded. These variables are not needed for building the model and prediction. We drop these variables and do not include them any further.</p>

<pre lang="r,"><code>library(dplyr)
trainData &lt;- select(trainData, -c(1:7))
testData &lt;- select(testData, -c(1:7))
</code></pre>

<p>While checking the data for missing values, we observe that lot of variables have missing values or other strings recorded as NAs. We first attempt to understand which variables and what percentage of them constitute NAs and then decide how to tackle the NA imputation.</p>

<pre lang="r,"><code># Converting all variables into numeric class to identify missing values.
  trainData[, 1:152] &lt;- sapply(trainData[, 1:152], function(x) as.numeric(as.character(x)))
  testData[, 1:152] &lt;- sapply(testData[, 1:152], function(x) as.numeric(as.character(x)))
# Summation of observations for each variable
  varsNAall &lt;- apply(trainData[, 1:152], 2, mean)
  varsNA1all &lt;- apply(trainData[, 1:152], 2, function(x) mean(x, na.rm = TRUE))
# If all observations are NA, we get mean as NaN
# Recording index for variables with all NAs 
  varIndNAall &lt;- grep("NaN", varsNA1all)
# Summation of observations for each variable for test data
  varsNAtestall &lt;- apply(testData[, 1:152], 2, mean)
  varsNAtest1all &lt;- apply(testData[, 1:152], 2, function(x) mean(x, na.rm = TRUE))
# Recording index for variables with all NAs for test data
  varIndNAtestall &lt;- grep("NaN", varsNAtest1all)
# We can also check whether the all NAs index for training set is a subset of that for the test set
## sum(varIndNAtestall %in% varIndNAall) == length(varIndNAall)
# Removing variables which are all NAs in the training set
  trainData &lt;- trainData[, -varIndNAall]
  testData &lt;- testData[, -varIndNAall]
# Summation of observations for remaining variables
  varsNA &lt;- apply(trainData[, 1:146], 2, mean)
  varsNA1 &lt;- apply(trainData[, 1:146], 2, function(x) mean(x, na.rm = TRUE))
# Recording index for variables with at least 1 NA
  varIndNA &lt;- which(varsNA %in% as.numeric(as.character("")))
# Summation of observations for remaining variables for test data
  varsNAtest &lt;- apply(testData[, 1:146], 2, mean)
  varsNA1test &lt;- apply(trainData[, 1:146], 2, function(x) mean(x, na.rm = TRUE))
# Recording index for variables with at least 1 NA in test data
  varIndNAtest &lt;- which(varsNAtest %in% as.numeric(as.character("")))
# Variables containing NAs for test and training data are identical
## identical(varIndNA, varIndNAtest) # Returns TRUE

# Separating the variables containing NAs to check %age of NA values
  dtySet &lt;- trainData[, varIndNA]
  sum(is.na(dtySet)) / (dim(dtySet)[1] * dim(dtySet)[2]) 
## Returns 98% NAs in total
  propNNAs &lt;- apply(dtySet, 2, function(x) sum(!is.na(x))/length(x))
#  max(propNNAs) 
## not more than 2% values Non-NAs for any of the variables
# We can remove these variables to get a cleaner dataset containing 52 variables and the categorical outcome
  clnSet &lt;- trainData[, -varIndNA]

# Separating the variables containing NAs to check %age of NA values in the test set
  dtySetTst &lt;- testData[, varIndNAtest]
  sum(is.na(dtySetTst)) / (dim(dtySetTst)[1] * dim(dtySetTst)[2]) 
## Returns 100% NAs
# We can remove these variables to get a cleaner dataset containing 52 variables and the problem id for the test set
  clnSetTst &lt;- testData[, -varIndNAtest]

## Imputation is not carried out for NAs as large number of similar values in each variable will introduce bias
</code></pre>

<p>The subsetting is now applied on the backed up raw datasets to obtain the cleaner dataset. The dataset used for building the model contains 52 variables and the classifier outcome.</p>

<pre lang="r,"><code>library(dplyr)
  trnDat &lt;- select(trainDataRD, -c(1:7))
  tstDat &lt;- select(testDataRD, -c(1:7))

  trnDat &lt;- trnDat[, -varIndNAall]
  tstDat &lt;- tstDat[, -varIndNAall]

  trnDat &lt;- trnDat[, -varIndNA]
  tstDat &lt;- tstDat[, -varIndNAtest]
</code></pre>

<h1>
<a id="building-the-prediction-model" class="anchor" href="#building-the-prediction-model" aria-hidden="true"><span class="octicon octicon-link"></span></a>Building the Prediction model</h1>

<p><em>Classification Trees and Random Forests are suitable for building prediction models for classification based outcomes</em>. For this assignment, Random forests is used as it provides high accuracy and can handle large number of variables without variable deletion.</p>

<p>'randomForest' is the function used to implement this task. The arguments provided to the randomForest function are the number of trees to be grown, number of variables randomly sampled at each split and the importance of predictors. </p>

<p>Number of trees can be set even higher but the error rates are more or less constant beyond 100 trees. Even with mtry = 3 pretty high accuracy is achieved. We want the importance of predictors to be assessed so setting the 'importance' argument as TRUE. </p>

<pre lang="r,"><code>library(randomForest)
  set.seed(1234)
  modRFfin &lt;- randomForest(classe ~ ., data = trnDat, ntree = 100, mtry = 3, importance = TRUE)
  modRFfin
</code></pre>

<h1>
<a id="cross-validation-and-out-of-sample-error" class="anchor" href="#cross-validation-and-out-of-sample-error" aria-hidden="true"><span class="octicon octicon-link"></span></a>Cross-validation and Out of sample error</h1>

<p>Each tree in a random forest is constructed using a different bootstrap sample from the original data. About one-third of the cases are left out of the bootstrap sample and not used in the construction of the kth tree. Hence, <strong>there is no need of cross-validation when using random forests</strong>. The out of bag (OOB) error rate is the estimate on the out of bag samples. In this way, a test set classification is obtained for each case in about one-third of the trees. So <strong>the OOB error rate is an unbiased estimate of the test set error</strong>. Based on our model, <strong>the expected out of sample error is less than 0.5%</strong>. </p>

<h1>
<a id="error-rate-and-variable-importance-plots" class="anchor" href="#error-rate-and-variable-importance-plots" aria-hidden="true"><span class="octicon octicon-link"></span></a>Error rate and Variable Importance plots</h1>

<p>Looking at how error rate behaves as the trees are built gives a good estimate of whether the solution is converging. We plot the error rates here and find that error rates for each of the outcome is close to 1% at around the 30th tree. OOB error rate is also plotted alongwith the error rate for each outcome.</p>

<p>It is also useful to look at the important variables as per our model. We have plotted the top 10 variables which are important w.r.t. the mean decrease in accuracy and the mean decrease in Gini Index.</p>

<pre lang="r,"><code>## Error rates plot
  plot(modRFfin, type = "l")
  legend("topright", colnames(modRFfin$err.rate), col=1:6, cex=0.8, fill=1:6)

## Plotting variable importance
  varImpPlot(modRFfin, n.var = 10, scale = FALSE)
</code></pre>

<h2>
<a id="submission-for-test-data" class="anchor" href="#submission-for-test-data" aria-hidden="true"><span class="octicon octicon-link"></span></a>Submission for test data</h2>

<p>The model is applied to the test cases to predict the outcomes using the predict function as shown below. <strong>Correct outcomes are predicted for each test case</strong>.</p>

<pre lang="r,"><code>ans &lt;- predict(modRFfin, newdata = tstDat[, -53])

pml_write_files = function(x){
  n = length(x)
  for(i in 1:n){
    filename = paste0("problem_id_",i,".txt")
    write.table(x[i],file=filename,quote=FALSE,row.names=FALSE,col.names=FALSE)
  }
}

pll_write_files(ans)
</code></pre>

<h2>
<a id="conclusions" class="anchor" href="#conclusions" aria-hidden="true"><span class="octicon octicon-link"></span></a>Conclusions</h2>

<p><em>A robust model was built using Random Forests and found to predict correctly on the test cases. Using this model, the expected out of sample error is less than 0.5%</em></p>
      </section>
    </div>

    <!-- FOOTER  -->
    <div id="footer_wrap" class="outer">
      <footer class="inner">
        <p class="copyright">Projectsubmission maintained by <a href="https://github.com/Aadityauppal">Aadityauppal</a></p>
        <p>Published with <a href="https://pages.github.com">GitHub Pages</a></p>
      </footer>
    </div>

    

  </body>
</html>
